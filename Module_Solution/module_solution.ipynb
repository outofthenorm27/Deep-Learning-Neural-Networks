{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789aa6a8",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning Models\n",
    "\n",
    "## 19.2.2 Build a Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import sklearn as skl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=78)\n",
    "\n",
    "# Creating a DataFrame with the dummy data\n",
    "df = pd.DataFrame(X, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "# Plotting the dummy data\n",
    "df.plot.scatter(x=\"Feature 1\", y=\"Feature 2\", c=\"Target\", colormap=\"winter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8453df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler instance\n",
    "X_scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras Sequential model\n",
    "nn_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our first Dense layer, including the input layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"relu\", input_dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c884ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer that uses a probability activation function\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3810cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the Sequential model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796eaa9",
   "metadata": {},
   "source": [
    "## 19.2.3 Train and Test a Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b251f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee221f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831770b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classification of a new set of blob data\n",
    "new_X, new_Y = make_blobs(n_samples=10, centers=2, n_features=2, random_state=78)\n",
    "new_X_scaled = X_scaler.transform(new_X)\n",
    "(nn_model.predict(new_X_scaled) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97539789",
   "metadata": {},
   "source": [
    "## 19.2.4 Nuances of Neural Networks on Nonlinear Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39560383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Creating dummy nonlinear data\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.08, random_state=78)\n",
    "\n",
    "# Transforming y_moons to a vertical vector\n",
    "y_moons = y_moons.reshape(-1, 1)\n",
    "\n",
    "# Creating a DataFrame to plot the nonlinear dummy data\n",
    "df_moons = pd.DataFrame(X_moons, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "df_moons[\"Target\"] = y_moons\n",
    "\n",
    "# Plot the nonlinear dummy data\n",
    "df_moons.plot.scatter(x=\"Feature 1\",y=\"Feature 2\", c=\"Target\",colormap=\"winter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04746e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "X_moon_train, X_moon_test, y_moon_train, y_moon_test = train_test_split(\n",
    "    X_moons, y_moons, random_state=78\n",
    ")\n",
    "\n",
    "# Create the scaler instance\n",
    "X_moon_scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_moon_scaler.fit(X_moon_train)\n",
    "\n",
    "# Scale the data\n",
    "X_moon_train_scaled = X_moon_scaler.transform(X_moon_train)\n",
    "X_moon_test_scaled = X_moon_scaler.transform(X_moon_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the nonlinear data\n",
    "model_moon = nn_model.fit(X_moon_train_scaled, y_moon_train, epochs=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(model_moon.history, index=range(1,len(model_moon.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123319ef",
   "metadata": {},
   "source": [
    "## 19.2.5 Create the Connective Tissue, the Multiple Neuron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our new Sequential model\n",
    "new_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the input and hidden layer\n",
    "number_inputs = 2\n",
    "number_hidden_nodes = 6\n",
    "\n",
    "new_model.add(tf.keras.layers.Dense(units=number_hidden_nodes, activation=\"relu\", input_dim=number_inputs))\n",
    "\n",
    "# Add the output layer that uses a probability activation function\n",
    "new_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6572fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model together and customize metrics\n",
    "new_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model to the training data\n",
    "new_fit_model = new_model.fit(X_moon_train_scaled, y_moon_train, epochs=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1bb22",
   "metadata": {},
   "source": [
    "## 19.3.3 Practice Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b865f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "\n",
    "# Read in our ramen data\n",
    "ramen_df = pd.read_csv(\"Resources/ramen-ratings.csv\")\n",
    "\n",
    "# Print out the Country value counts\n",
    "country_counts = ramen_df.Country.value_counts()\n",
    "country_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the value counts\n",
    "country_counts.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which values to replace\n",
    "replace_countries = list(country_counts[country_counts < 100].index)\n",
    "\n",
    "# Replace in DataFrame\n",
    "for country in replace_countries:\n",
    "    ramen_df.Country = ramen_df.Country.replace(country,\"Other\")\n",
    "\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "ramen_df.Country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab46f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OneHotEncoder instance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit the encoder and produce encoded DataFrame\n",
    "encode_df = pd.DataFrame(enc.fit_transform(ramen_df.Country.values.reshape(-1,1)))\n",
    "\n",
    "# Rename encoded columns\n",
    "encode_df.columns = enc.get_feature_names(['Country'])\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d98399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames together and drop the Country column\n",
    "ramen_df.merge(encode_df,left_index=True,right_index=True).drop(\"Country\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3a1d3",
   "metadata": {},
   "source": [
    "## 19.3.4 Span the Gap Using Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40149a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read in our dataset\n",
    "hr_df = pd.read_csv(\"Resources/hr_dataset.csv\")\n",
    "hr_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the StandardScaler\n",
    "scaler.fit(hr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f089b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaled_data = scaler.transform(hr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scaled data\n",
    "transformed_scaled_data = pd.DataFrame(scaled_data, columns=hr_df.columns)\n",
    "transformed_scaled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72e858",
   "metadata": {},
   "source": [
    "## 19.4.2 Real Data, Real Practice: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "attrition_df = pd.read_csv('Resources/HR-Employee-Attrition.csv')\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our categorical variable list\n",
    "attrition_cat = attrition_df.dtypes[attrition_df.dtypes == \"object\"].index.tolist()\n",
    "attrition_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd6b52",
   "metadata": {},
   "source": [
    "## 19.4.3 Real Data, Real Practice: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique values in each column\n",
    "attrition_df[attrition_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(attrition_df[attrition_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(attrition_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d837f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "attrition_df = attrition_df.merge(encode_df,left_index=True, right_index=True)\n",
    "attrition_df = attrition_df.drop(attrition_cat,1)\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = attrition_df[\"Attrition_Yes\"].values\n",
    "X = attrition_df.drop([\"Attrition_Yes\",\"Attrition_No\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0979cc6",
   "metadata": {},
   "source": [
    "## 19.4.4 Real Data, Real Practice: Deep Learning Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be15537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72d96e",
   "metadata": {},
   "source": [
    "## 19.4.5 Real Data, Real Practice: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a56277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b9913",
   "metadata": {},
   "source": [
    "## 19.5.2 Logistic Regression Vs. a Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "diabetes_df = pd.read_csv('Resources/diabetes.csv')\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove diabetes outcome target from features data\n",
    "y = diabetes_df.Outcome\n",
    "X = diabetes_df.drop(columns=\"Outcome\")\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numerical data for neural network\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ca7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "log_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = log_classifier.predict(X_test)\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic neural network model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\", input_dim=8))\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453e32c",
   "metadata": {},
   "source": [
    "## 19.5.3 Support Vector Machine Vs. Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "tele_df = pd.read_csv('Resources/bank_telemarketing.csv')\n",
    "tele_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f51214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our categorical variable list\n",
    "tele_cat = tele_df.dtypes[tele_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "tele_df[tele_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(tele_df[tele_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(tele_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c638ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "tele_df = tele_df.merge(encode_df,left_index=True, right_index=True)\n",
    "tele_df = tele_df.drop(tele_cat,1)\n",
    "tele_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove loan status target from features data\n",
    "y = tele_df.Subscribed_yes.values\n",
    "X = tele_df.drop(columns=[\"Subscribed_no\",\"Subscribed_yes\"]).values\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c682d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1678e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5004455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ae464",
   "metadata": {},
   "source": [
    "## 19.5.4 Random Forest Vs. Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "loans_df = pd.read_csv('Resources/loan_status.csv')\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16152141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our categorical variable list\n",
    "loans_cat = loans_df.dtypes[loans_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "loans_df[loans_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique value counts to see if binning is required\n",
    "loans_df.Years_in_current_job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(loans_df[loans_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(loans_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "loans_df = loans_df.merge(encode_df,left_index=True, right_index=True)\n",
    "loans_df = loans_df.drop(loans_cat,1)\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb82420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove loan status target from features data\n",
    "y = loans_df.Loan_Status_Fully_Paid\n",
    "X = loans_df.drop(columns=[\"Loan_Status_Fully_Paid\",\"Loan_Status_Not_Paid\"])\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238154e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 24\n",
    "hidden_nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989239a",
   "metadata": {},
   "source": [
    "## 19.6.1 Checkpoints Are Not Just for Video Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da86f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e394a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Restore the model weights\n",
    "nn_new.load_weights(\"checkpoints/weights.100.hdf5\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fc206",
   "metadata": {},
   "source": [
    "## 19.6.2 For Best Results, Please Save After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn_new.save(\"trained_attrition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c983ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model to a new object\n",
    "nn_imported = tf.keras.models.load_model('trained_attrition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the completed model using the test data\n",
    "model_loss, model_accuracy = nn_imported.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
